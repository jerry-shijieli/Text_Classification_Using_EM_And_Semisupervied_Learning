import numpy as npfrom copy import deepcopyfrom scipy.sparse import csr_matrix, vstackfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.naive_bayes import GaussianNBfrom scipy.linalg import get_blas_funcsclass Semi_EM_MultinomialNB():    """    Naive Bayes classifier for multinomial models for semi-supervised learning.        Use both labeled and unlabeled data to train NB classifier, update parameters    using unlabeled data, and all data to evaluate performance of classifier. Optimize    classifier using Expectation-Maximization algorithm.    """    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, max_iter=5, tol=1e-6):        self.alpha = alpha        self.fit_prior = fit_prior        self.class_prior = class_prior        self.clf = MultinomialNB(alpha=self.alpha, fit_prior=self.fit_prior, class_prior=self.class_prior)        self.log_lkh = -np.inf # log likelihood        self.max_iter = max_iter # max number of EM iterations        self.tol = tol # tolerance of log likelihood increment    def fit(self, X_l, y_l, X_u):        n_ul_docs = X_u.shape[0] # number of unlabeled samples        # initialization (n_docs = n_ul_docs)        clf = deepcopy(self.clf)# build new copy of classifier        clf.fit(X_l, y_l) # use labeled data only to initialize classifier parameters        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]        lp_d_c = get_blas_funcs("gemm", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray())         lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]        expectation = get_blas_funcs("gemm", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace()         self.clf = deepcopy(clf)        self.log_lkh = expectation        print expectation        # Loop until log likelihood does not improve        iter_count = 0 # count EM iteration        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):        # while (iter_count<self.max_iter):            iter_count += 1            print iter_count # debug            # E-step: Estimate class membership of unlabeled documents            y_u = clf.predict(X_u)            # M-step: Re-estimate classifier parameters            X = vstack([X_l, X_u])            y = np.concatenate((y_l, y_u), axis=0)            clf.fit(X, y)            # check convergence: update log likelihood            p_c_d = clf.predict_proba(X_u)            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]            b_w_d = (X_u > 0) # words in each document            lp_d_c = get_blas_funcs("gemm", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray())             lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]            expectation = get_blas_funcs("gemm", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace()             print expectation            if (expectation-self.log_lkh >= self.tol):                prev_log_lkh = self.log_lkh                self.log_lkh = expectation                self.clf = deepcopy(clf)            else:                break        return self    def partial_fit(self, X_l, y_l, X_u):        pass    def predict(self, X):        return self.clf.predict(X)    def score(self, X, y):        pass